package provider

import (
	"context"
	"fmt"
	"time"

	"github.com/hashicorp/terraform-plugin-log/tflog"
	"github.com/hashicorp/terraform-plugin-sdk/v2/diag"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/validation"
	"google.golang.org/protobuf/types/known/fieldmaskpb"

	"github.com/bytebase/terraform-provider-bytebase/api"
	"github.com/bytebase/terraform-provider-bytebase/provider/internal"

	v1pb "github.com/bytebase/bytebase/backend/generated-go/v1"
)

// defaultProj is the default project name.
var defaultProj = fmt.Sprintf("%sdefault", internal.ProjectNamePrefix)

func resourceProjct() *schema.Resource {
	return &schema.Resource{
		Description:          "The project resource.",
		CreateWithoutTimeout: resourceProjectCreate,
		ReadWithoutTimeout:   resourceProjectRead,
		UpdateWithoutTimeout: resourceProjectUpdate,
		DeleteContext:        resourceProjectDelete,
		Importer: &schema.ResourceImporter{
			StateContext: schema.ImportStatePassthroughContext,
		},
		Schema: map[string]*schema.Schema{
			"resource_id": {
				Type:         schema.TypeString,
				Required:     true,
				ValidateFunc: internal.ResourceIDValidation,
				Description:  "The project unique resource id. Cannot change this after created.",
			},
			"title": {
				Type:         schema.TypeString,
				Required:     true,
				ValidateFunc: validation.StringIsNotEmpty,
				Description:  "The project title.",
			},
			"name": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: "The project full name in projects/{resource id} format.",
			},
			"allow_modify_statement": {
				Type:        schema.TypeBool,
				Optional:    true,
				Computed:    true,
				Description: "Allow modifying statement after issue is created.",
			},
			"auto_resolve_issue": {
				Type:        schema.TypeBool,
				Optional:    true,
				Computed:    true,
				Description: "Enable auto resolve issue.",
			},
			"enforce_issue_title": {
				Type:        schema.TypeBool,
				Optional:    true,
				Computed:    true,
				Description: "Enforce issue title created by user instead of generated by Bytebase.",
			},
			"auto_enable_backup": {
				Type:        schema.TypeBool,
				Optional:    true,
				Computed:    true,
				Description: "Whether to automatically enable backup.",
			},
			"skip_backup_errors": {
				Type:        schema.TypeBool,
				Optional:    true,
				Computed:    true,
				Description: "Whether to skip backup errors and continue the data migration.",
			},
			"allow_self_approval": {
				Type:        schema.TypeBool,
				Optional:    true,
				Computed:    true,
				Description: "Whether to allow the issue creator to self-approve the issue.",
			},
			"postgres_database_tenant_mode": {
				Type:        schema.TypeBool,
				Optional:    true,
				Computed:    true,
				Description: "Whether to enable the database tenant mode for PostgreSQL. If enabled, the issue will be created with the pre-appended \"set role <db_owner>\" statement.",
			},
			"execution_retry_policy": {
				Type:        schema.TypeInt,
				Optional:    true,
				Computed:    true,
				Description: "The maximum number of retries for the lock timeout issue.",
			},
			"ci_sampling_size": {
				Type:        schema.TypeInt,
				Optional:    true,
				Computed:    true,
				Description: "The maximum databases of rows to sample during CI data validation. Without specification, sampling is disabled, resulting in a full validation.",
			},
			"parallel_tasks_per_rollout": {
				Type:        schema.TypeInt,
				Optional:    true,
				Computed:    true,
				Description: "The maximum number of parallel tasks to run during the rollout.",
			},
			"databases": getDatabasesSchema(false),
			"webhooks":  getWebhooksSchema(false),
		},
	}
}

func resourceProjectCreate(ctx context.Context, d *schema.ResourceData, m interface{}) diag.Diagnostics {
	c := m.(api.Client)

	projectID := d.Get("resource_id").(string)
	projectName := fmt.Sprintf("%s%s", internal.ProjectNamePrefix, projectID)
	project := &v1pb.Project{
		Name:                       projectName,
		Title:                      d.Get("title").(string),
		State:                      v1pb.State_ACTIVE,
		AllowModifyStatement:       d.Get("allow_modify_statement").(bool),
		AutoResolveIssue:           d.Get("auto_resolve_issue").(bool),
		EnforceIssueTitle:          d.Get("enforce_issue_title").(bool),
		AutoEnableBackup:           d.Get("auto_enable_backup").(bool),
		SkipBackupErrors:           d.Get("skip_backup_errors").(bool),
		AllowSelfApproval:          d.Get("allow_self_approval").(bool),
		PostgresDatabaseTenantMode: d.Get("postgres_database_tenant_mode").(bool),
		ExecutionRetryPolicy: &v1pb.Project_ExecutionRetryPolicy{
			MaximumRetries: int32(d.Get("execution_retry_policy").(int)),
		},
		CiSamplingSize:          int32(d.Get("ci_sampling_size").(int)),
		ParallelTasksPerRollout: int32(d.Get("parallel_tasks_per_rollout").(int)),
	}

	existedProject, err := c.GetProject(ctx, projectName)
	if err != nil {
		tflog.Debug(ctx, fmt.Sprintf("get project %s failed with error: %v", projectName, err))
	}

	rawConfig := d.GetRawConfig()
	updateMasks := []string{}
	if config := rawConfig.GetAttr("allow_modify_statement"); !config.IsNull() {
		updateMasks = append(updateMasks, "allow_modify_statement")
	}
	if config := rawConfig.GetAttr("auto_resolve_issue"); !config.IsNull() {
		updateMasks = append(updateMasks, "auto_resolve_issue")
	}
	if config := rawConfig.GetAttr("enforce_issue_title"); !config.IsNull() {
		updateMasks = append(updateMasks, "enforce_issue_title")
	}
	if config := rawConfig.GetAttr("auto_enable_backup"); !config.IsNull() {
		updateMasks = append(updateMasks, "auto_enable_backup")
	}
	if config := rawConfig.GetAttr("skip_backup_errors"); !config.IsNull() {
		updateMasks = append(updateMasks, "skip_backup_errors")
	}
	if config := rawConfig.GetAttr("allow_self_approval"); !config.IsNull() {
		updateMasks = append(updateMasks, "allow_self_approval")
	}
	if config := rawConfig.GetAttr("postgres_database_tenant_mode"); !config.IsNull() {
		updateMasks = append(updateMasks, "postgres_database_tenant_mode")
	}
	if config := rawConfig.GetAttr("execution_retry_policy"); !config.IsNull() {
		updateMasks = append(updateMasks, "execution_retry_policy")
	}
	if config := rawConfig.GetAttr("ci_sampling_size"); !config.IsNull() {
		updateMasks = append(updateMasks, "ci_sampling_size")
	}
	if config := rawConfig.GetAttr("parallel_tasks_per_rollout"); !config.IsNull() {
		updateMasks = append(updateMasks, "parallel_tasks_per_rollout")
	}

	var diags diag.Diagnostics
	if existedProject != nil && err == nil {
		diags = append(diags, diag.Diagnostic{
			Severity: diag.Warning,
			Summary:  "Project already exists",
			Detail:   fmt.Sprintf("Project %s already exists, try to exec the update operation", projectName),
		})

		if existedProject.State == v1pb.State_DELETED {
			diags = append(diags, diag.Diagnostic{
				Severity: diag.Warning,
				Summary:  "Project is deleted",
				Detail:   fmt.Sprintf("Project %s already deleted, try to undelete the project", projectName),
			})
			if _, err := c.UndeleteProject(ctx, projectName); err != nil {
				diags = append(diags, diag.Diagnostic{
					Severity: diag.Error,
					Summary:  "Failed to undelete project",
					Detail:   fmt.Sprintf("Undelete project %s failed, error: %v", projectName, err),
				})
				return diags
			}
		}

		if project.Title != existedProject.Title {
			updateMasks = append(updateMasks, "title")
		}
	} else {
		if _, err := c.CreateProject(ctx, projectID, project); err != nil {
			return diag.FromErr(err)
		}
	}

	if len(updateMasks) > 0 {
		if _, err := c.UpdateProject(ctx, project, updateMasks); err != nil {
			diags = append(diags, diag.Diagnostic{
				Severity: diag.Error,
				Summary:  "Failed to update project",
				Detail:   fmt.Sprintf("Update project %s failed, error: %v", projectName, err),
			})
			return diags
		}
	}

	d.SetId(projectName)

	if diag := updateDatabasesInProject(ctx, d, c, d.Id()); diag != nil {
		diags = append(diags, diag...)
		return diags
	}

	if diag := updateWebhooksInProject(ctx, d, c, d.Id()); diag != nil {
		diags = append(diags, diag...)
		return diags
	}

	tflog.Debug(ctx, "[upsert project] start reading project", map[string]interface{}{
		"project": projectName,
	})

	diag := resourceProjectRead(ctx, d, m)
	if diag != nil {
		diags = append(diags, diag...)
	}

	tflog.Debug(ctx, "[upsert project] upsert project finished", map[string]interface{}{
		"project": projectName,
	})

	return diags
}

func resourceProjectUpdate(ctx context.Context, d *schema.ResourceData, m interface{}) diag.Diagnostics {
	if d.HasChange("resource_id") {
		return diag.Errorf("cannot change the resource id")
	}

	c := m.(api.Client)
	projectName := d.Id()

	existedProject, err := c.GetProject(ctx, projectName)
	if err != nil {
		return diag.Errorf("get project %s failed with error: %v", projectName, err)
	}

	var diags diag.Diagnostics
	if existedProject.State == v1pb.State_DELETED {
		diags = append(diags, diag.Diagnostic{
			Severity: diag.Warning,
			Summary:  "Project is deleted",
			Detail:   fmt.Sprintf("Project %s already deleted, try to undelete the project", projectName),
		})
		if _, err := c.UndeleteProject(ctx, projectName); err != nil {
			diags = append(diags, diag.Diagnostic{
				Severity: diag.Error,
				Summary:  "Failed to undelete project",
				Detail:   fmt.Sprintf("Undelete project %s failed, error: %v", projectName, err),
			})
			return diags
		}
	}

	paths := []string{}
	if d.HasChange("title") {
		paths = append(paths, "title")
	}
	if d.HasChange("allow_modify_statement") {
		paths = append(paths, "allow_modify_statement")
	}
	if d.HasChange("auto_resolve_issue") {
		paths = append(paths, "auto_resolve_issue")
	}
	if d.HasChange("enforce_issue_title") {
		paths = append(paths, "enforce_issue_title")
	}
	if d.HasChange("auto_enable_backup") {
		paths = append(paths, "auto_enable_backup")
	}
	if d.HasChange("skip_backup_errors") {
		paths = append(paths, "skip_backup_errors")
	}
	if d.HasChange("allow_self_approval") {
		paths = append(paths, "allow_self_approval")
	}
	if d.HasChange("postgres_database_tenant_mode") {
		paths = append(paths, "postgres_database_tenant_mode")
	}
	if d.HasChange("execution_retry_policy") {
		paths = append(paths, "execution_retry_policy")
	}
	if d.HasChange("ci_sampling_size") {
		paths = append(paths, "ci_sampling_size")
	}
	if d.HasChange("parallel_tasks_per_rollout") {
		paths = append(paths, "parallel_tasks_per_rollout")
	}

	if len(paths) > 0 {
		if _, err := c.UpdateProject(ctx, &v1pb.Project{
			Name:                       projectName,
			Title:                      d.Get("title").(string),
			State:                      v1pb.State_ACTIVE,
			AllowModifyStatement:       d.Get("allow_modify_statement").(bool),
			AutoResolveIssue:           d.Get("auto_resolve_issue").(bool),
			EnforceIssueTitle:          d.Get("enforce_issue_title").(bool),
			AutoEnableBackup:           d.Get("auto_enable_backup").(bool),
			SkipBackupErrors:           d.Get("skip_backup_errors").(bool),
			AllowSelfApproval:          d.Get("allow_self_approval").(bool),
			PostgresDatabaseTenantMode: d.Get("postgres_database_tenant_mode").(bool),
			ExecutionRetryPolicy: &v1pb.Project_ExecutionRetryPolicy{
				MaximumRetries: int32(d.Get("execution_retry_policy").(int)),
			},
			CiSamplingSize:          int32(d.Get("ci_sampling_size").(int)),
			ParallelTasksPerRollout: int32(d.Get("parallel_tasks_per_rollout").(int)),
		}, paths); err != nil {
			diags = append(diags, diag.FromErr(err)...)
			return diags
		}
	}

	if d.HasChange("databases") {
		if diag := updateDatabasesInProject(ctx, d, c, d.Id()); diag != nil {
			diags = append(diags, diag...)
			return diags
		}
	}
	if d.HasChange("webhooks") {
		if diag := updateWebhooksInProject(ctx, d, c, d.Id()); diag != nil {
			diags = append(diags, diag...)
			return diags
		}
	}

	diag := resourceProjectRead(ctx, d, m)
	if diag != nil {
		diags = append(diags, diag...)
	}

	return diags
}

func resourceProjectRead(ctx context.Context, d *schema.ResourceData, m interface{}) diag.Diagnostics {
	c := m.(api.Client)
	projectName := d.Id()

	project, err := c.GetProject(ctx, projectName)
	if err != nil {
		return diag.FromErr(err)
	}

	resp := setProject(ctx, c, d, project)
	tflog.Debug(ctx, "[read project] read project finished", map[string]interface{}{
		"project": project.Name,
	})

	return resp
}

func resourceProjectDelete(ctx context.Context, d *schema.ResourceData, m interface{}) diag.Diagnostics {
	c := m.(api.Client)
	projectName := d.Id()

	// Warning or errors can be collected in a slice type
	var diags diag.Diagnostics

	if err := c.DeleteProject(ctx, projectName); err != nil {
		return diag.FromErr(err)
	}

	d.SetId("")

	return diags
}

const batchSize = 100

func updateDatabasesInProject(ctx context.Context, d *schema.ResourceData, client api.Client, projectName string) diag.Diagnostics {
	rawConfig := d.GetRawConfig()
	if config := rawConfig.GetAttr("databases"); config.IsNull() {
		return nil
	}

	databases, err := client.ListDatabase(ctx, projectName, &api.DatabaseFilter{}, true)
	if err != nil {
		return diag.Errorf("failed to list database with error: %v", err.Error())
	}
	existedDBMap := map[string]*v1pb.Database{}
	for _, db := range databases {
		existedDBMap[db.Name] = db
	}

	rawSet, ok := d.Get("databases").(*schema.Set)
	if !ok {
		return nil
	}
	updatedDBMap := map[string]*v1pb.Database{}
	batchTransferDatabases := []*v1pb.UpdateDatabaseRequest{}
	for _, raw := range rawSet.List() {
		dbName := raw.(string)
		if _, _, err := internal.GetInstanceDatabaseID(dbName); err != nil {
			return diag.Errorf("invalid database full name: %v", err.Error())
		}

		updatedDBMap[dbName] = &v1pb.Database{
			Name:    dbName,
			Project: projectName,
		}
		_, ok := existedDBMap[dbName]
		if !ok {
			// new assigned database
			batchTransferDatabases = append(batchTransferDatabases, &v1pb.UpdateDatabaseRequest{
				Database: updatedDBMap[dbName],
				UpdateMask: &fieldmaskpb.FieldMask{
					Paths: []string{"project"},
				},
			})
		} else {
			delete(existedDBMap, dbName)
		}
	}

	tflog.Debug(ctx, "[transfer databases] batch transfer databases to project", map[string]interface{}{
		"project":   projectName,
		"databases": len(batchTransferDatabases),
	})

	for i := 0; i < len(batchTransferDatabases); i += batchSize {
		end := i + batchSize
		if end > len(batchTransferDatabases) {
			end = len(batchTransferDatabases)
		}
		batch := batchTransferDatabases[i:end]
		startTime := time.Now()

		if _, err := client.BatchUpdateDatabases(ctx, &v1pb.BatchUpdateDatabasesRequest{
			Requests: batch,
			Parent:   "instances/-",
		}); err != nil {
			return diag.Errorf("failed to assign databases to project %s with error: %v", projectName, err.Error())
		}

		tflog.Debug(ctx, "[transfer databases]", map[string]interface{}{
			"count":   end + 1 - i,
			"project": projectName,
			"ms":      time.Since(startTime).Milliseconds(),
		})
	}

	if len(existedDBMap) > 0 {
		tflog.Debug(ctx, "[transfer databases] batch unassign databases", map[string]interface{}{
			"project":   projectName,
			"databases": len(existedDBMap),
		})

		startTime := time.Now()
		unassignDatabases := []*v1pb.UpdateDatabaseRequest{}
		for _, db := range existedDBMap {
			// move db to default project
			db.Project = defaultProj
			unassignDatabases = append(unassignDatabases, &v1pb.UpdateDatabaseRequest{
				Database: db,
				UpdateMask: &fieldmaskpb.FieldMask{
					Paths: []string{"project"},
				},
			})
		}
		if _, err := client.BatchUpdateDatabases(ctx, &v1pb.BatchUpdateDatabasesRequest{
			Requests: unassignDatabases,
			Parent:   "instances/-",
		}); err != nil {
			return diag.Errorf("failed to move databases to default project with error: %v", err.Error())
		}
		tflog.Debug(ctx, "[unassign databases]", map[string]interface{}{
			"count": len(unassignDatabases),
			"ms":    time.Since(startTime).Milliseconds(),
		})
	}

	return nil
}

func updateWebhooksInProject(ctx context.Context, d *schema.ResourceData, client api.Client, projectName string) diag.Diagnostics {
	rawConfig := d.GetRawConfig()
	if config := rawConfig.GetAttr("webhooks"); config.IsNull() {
		return nil
	}

	project, err := client.GetProject(ctx, projectName)
	if err != nil {
		return diag.FromErr(err)
	}

	existedWebhookMap := map[string]*v1pb.Webhook{}
	for _, webhook := range project.Webhooks {
		existedWebhookMap[webhook.Name] = webhook
	}

	for _, w := range d.Get("webhooks").([]interface{}) {
		rawWebhook := w.(map[string]interface{})
		webhook := &v1pb.Webhook{
			Name:          rawWebhook["name"].(string),
			Title:         rawWebhook["title"].(string),
			Url:           rawWebhook["url"].(string),
			DirectMessage: rawWebhook["direct_message"].(bool),
			Type:          v1pb.Webhook_Type(v1pb.Webhook_Type_value[rawWebhook["type"].(string)]),
		}
		notificationTypes := rawWebhook["notification_types"].(*schema.Set)
		for _, n := range notificationTypes.List() {
			webhook.NotificationTypes = append(webhook.NotificationTypes, v1pb.Activity_Type(v1pb.Activity_Type_value[n.(string)]))
		}

		if v, ok := existedWebhookMap[webhook.Name]; ok && v.Type == webhook.Type {
			// Not support change the webhook type.
			if _, err := client.UpdateProjectWebhook(ctx, webhook, []string{
				"title", "url", "notification_type", "direct_message",
			}); err != nil {
				return diag.Errorf("failed to update webhook %s with error: %v", webhook.Name, err.Error())
			}
			delete(existedWebhookMap, webhook.Name)
		} else {
			if _, err := client.CreateProjectWebhook(ctx, project.Name, webhook); err != nil {
				return diag.Errorf("failed to create webhook in project %s with error: %v", project.Name, err.Error())
			}
		}
	}

	for webhookName := range existedWebhookMap {
		if err := client.DeleteProjectWebhook(ctx, webhookName); err != nil {
			return diag.Errorf("failed to delete webhook %s with error: %v", webhookName, err.Error())
		}
	}

	return nil
}
